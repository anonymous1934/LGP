# Local & Global Patching (LGP): Positionâ€‘Free Visual Backbones

> ğŸš€ **Unbelievable yet simple:** On **ImageNetâ€‘100**, adding LGP to the **most basic** backbones yields **+8.40 Topâ€‘1** on **ViTâ€‘T** and **+5.31 Topâ€‘1** on **ViMâ€‘T** with **nearâ€‘zero extra compute** and only a **tiny plugâ€‘in** at patch embedding.  
> ğŸ‘‰ We release **pretrained weights** and **training logs** for both models:  
> â€¢ **ViTâ€‘T + LGP (INâ€‘100)** â€” [Weights](<link-to-vit-t-weights>) Â· [Logs](<link-to-vit-t-logs>)  
> â€¢ **ViMâ€‘T + LGP (INâ€‘100)** â€” [Weights](<link-to-vim-t-weights>) Â· [Logs](<link-to-vim-t-logs>)

**TL;DR.** LGP removes **all position embeddings** in ViT/ViM by enriching each patch token with **multiâ€‘band global wavelet features**. Every token carries an implicit â€œposition signature,â€ making the encoder **robust to patch order** and often **more accurate**â€”with **negligible compute overhead**. *(Paper link: <link-to-paper>)*

<p align="center">
  <img src="assets/teaser_lgp.png" alt="LGP vs. traditional patching (teaser)" width="85%">
</p>

*Teaser.* Conventional patching discards spatial structure and relies on learned positional channels later; **Localâ€‘Global Patching** preserves structure by fusing **global multiâ€‘band features** into each patch **at tokenization time**. No separate position learning is required.

---

## Table of Contents
- [Highlights](#highlights)
- [Method at a Glance](#method-at-a-glance)
- [Results](#results)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Dropâ€‘in Integration (PyTorch Pseudocode)](#drop-in-integration-pytorch-pseudocode)
- [Crossâ€‘Framework Usage](#cross-framework-usage)
- [Project Layout](#project-layout)
- [FAQ](#faq)
- [Citation](#citation)
- [License](#license)
- [Acknowledgements](#acknowledgements)

---

## Highlights

- **Positionâ€‘free tokens.** LGP equips every patch with a compact summary of the **whole image** across multiple frequency bands, making the encoder **orderâ€‘robust** (highly tolerant to patch permutations).  
- **Plugâ€‘andâ€‘play.** Add a lightweight **global wavelet branch** + **Adaptive Fusion Block (AFB)** at the **patch embedding** stage; the rest of ViT/ViM remains unchanged.  
- **Consistent gains.** On ImageNetâ€‘100, the smallest models benefit mostâ€”e.g., **ViTâ€‘T +8.40** and **ViMâ€‘T +5.31** topâ€‘1. On ImageNetâ€‘1K, ViTâ€‘S **+1.05** and ViMâ€‘S **+1.39** without retuning.  
- **Negligible overhead.** For typical configs (e.g., `d â‰¤ 768`, patch size 16), LGP adds **<5% FLOPs** in practice and scales **linearly** with image area.

---

## Method at a Glance

**Two synchronized paths:**
1) **Local** â€” standard ViT/ViM patch embedding to create local tokens.  
2) **Global** â€” an `n`â€‘level **2â€‘D waveletâ€‘packet transform** (WPT) of the full image yields **4â¿** subâ€‘bands; subâ€‘bands are channelâ€‘concatenated, projected with `1Ã—1` conv, and refined by **depthwiseâ€‘separable** convs to align with the patch grid.  
3) **Adaptive Fusion Block (AFB)** â€” a learnable **channel gate** blends local and global signals; three depthwiseâ€‘separable convs refine the mixture. The fused tokens already contain multiâ€‘scale semantics and **implicit positional cues**, so encoders need **no absolute/relative position embeddings**.

**Injective positional signature (intuition).** Because wavelet bases are spatially localized, the spectral vector aggregated for patch `(i, j)` is a distinctive signature of its location, yielding strong **order robustness** without explicit positional channels.

**Recommended depth.** Choose `n` so that **`4^n = P_h Ã— P_w`**, where `P_h Ã— P_w` is the patch grid. For square grids (`P_h = P_w = P`), this reduces to **`n = logâ‚‚ P`**â€”enabling patchâ€‘wise spectral pooling.

---

## Results

**ImageNetâ€‘100** (*scaling study*)

| Backbone | Params (M) | GFLOPs | Topâ€‘1 â†‘ |
|---|---:|---:|---:|
| **ViMâ€‘T** | 6.42 â†’ 6.65 | 0.16 â†’ 0.21 | **75.10 â†’ 80.41** (+5.31) |
| **ViMâ€‘S** | 23.91 â†’ 24.58 | 0.33 â†’ 0.47 | **79.12 â†’ 84.50** (+5.38) |
| **ViMâ€‘B** | 92.05 â†’ 94.28 | 0.65 â†’ 1.10 | **83.60 â†’ 86.33** (+2.73) |
| **ViTâ€‘T** | 5.72 â†’ 6.06 | 0.91 â†’ 0.98 | **67.64 â†’ 76.04** (+8.40) |
| **ViTâ€‘S** | 22.05 â†’ 23.18 | 3.22 â†’ 3.46 | **74.60 â†’ 80.34** (+5.74) |
| **ViTâ€‘B** | 86.57 â†’ 90.59 | 12.02 â†’ 12.84 | **76.23 â†’ 80.62** (+4.39) |

**Robustness to shuffling.** LGP sharply reduces sensitivity to **patch order** and **band order** (e.g., ViMâ€‘S drop: **âˆ’12.66% â†’ âˆ’0.12%**; ViTâ€‘S: **âˆ’4.29% â†’ âˆ’0.09%**).

**ImageNetâ€‘1K.** Outâ€‘ofâ€‘theâ€‘box, LGP yields **~+1%** absolute topâ€‘1 at this scale (no extra tuning).

> ğŸ”— **Artifacts.** We provide **pretrained weights** and **training logs** for **ViTâ€‘T + LGP** and **ViMâ€‘T + LGP** on ImageNetâ€‘100:  
> â€¢ ViTâ€‘T + LGP â€” [Weights](<link-to-vit-t-weights>) Â· [Logs](<link-to-vit-t-logs>)  
> â€¢ ViMâ€‘T + LGP â€” [Weights](<link-to-vim-t-weights>) Â· [Logs](<link-to-vim-t-logs>)
